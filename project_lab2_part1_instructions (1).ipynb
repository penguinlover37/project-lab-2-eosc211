{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Lab 2: Part 1\n## Core Functions and Initial Figures\n___ \n\n\n\n### A) Download the necessary data files\n    \nDownload the following data files from the `Data` folder inside the wk13 lab folder on Canvas and put them in a local or hub folder called `Data` in your own working folder for the project lab.  \n- `IRIS_eq_010100_112422_mag4.csv`\n- `m_coasts.csv`\n- `all_boundaries.csv`\n\nSee section **2. Project Lab 2 Data** in the `project_lab2_background.ipynb` reading for file format details and how to provide the appropriate path (e.g. `./Data/m_coasts.csv`) to load them properly.\n","metadata":{},"id":"e02925de"},{"cell_type":"markdown","source":"___ \n\n### B) Make a module `earthquake_fns`\n\nYou will create a new python (`.py`) file called `earthquake_fns.py` which contains the four functions detailed below. \n\n<!-- Your python file *must* contain the following `import` statements: \n\n```python\nimport numpy as np\nfrom datetime import datetime  # to parse dates and times\nimport pandas as pd            # read, write and rearrange tabular datasets\n``` -->\n\nMake sure that your `.py` file can be imported and run without error by importing the file as a module from a new notebook. You will use this module for the remainder of the project lab and will add additional functions to it in **Part 2**. \n> e.g. `import earthquake_fns as quake` will import your module, naming it `quake` in your notebook, so you can call e.g. `get_coastlines` with `quake.get_coastlines(...)`.\n\n\n<div class=\"alert alert-block alert-info\">\n    \n**Note**: There are many lines of code that you can use to help you write the functions involving reading / manipulating `DataFrame` objects in the Week 13 notebook `wk13_pandas_getfamiliar.ipynb`.\n\n</div> \n\n ","metadata":{},"id":"fd7db7f7-b50c-43da-a045-891640016e4b"},{"cell_type":"markdown","source":"### function 1:  `get_coastlines`\n\n**Code:** The body of `get_coastlines` should \n\n- take as input a `.csv` file containing two columns (column 1: longitudes of the world's coastlines, and column 2: latitudes of the world's coastlines) and read these columns into individual 1D arrays. \n- The function should also raise an `IOError` and exit with a helpful error message if an exception is encountered inside of the function. \n\n<!-- Note:  the file contains pairs `NaN` values to break up different coastlines -->\n\n**Usage:**\n```python\nlon_coast, lat_coast = get_coastlines(coasts_file)  \n```\n**Inputs:** \n> -  `coasts_file`:  file containing coastline lon, lat coordinates:  column 1 = longitudes and column 2 = latitudes.\n\n**Outputs:** \n> - *lon_coast*: an array of longitudes along world coastlines.\n> - *lat_coast*: an array of latitudes along world coastlines.\n\n***Note**: all inputs and outputs must be provided in the exact order as above. Output variable names in italics are suggested names.* ","metadata":{},"id":"7a9f4ba5-087a-4a28-863b-864f4848e40f"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntestmf = \"./m_coasts.csv\"\n# def get_coastlines(coasts_file):\n#     try:\n#         df = pd.read_csv(coasts_file)\n#         lon_coast = df.iloc[:,0]\n#         lat_coast = df.iloc[:,1]\n#     except:\n#         raise IOError\n#     return lon_coast, lat_coast\n\n# get_coastlines(testmf)","metadata":{"trusted":true,"tags":[]},"execution_count":1,"outputs":[],"id":"569f2816-c872-4dd0-9cc2-f843e82d145d"},{"cell_type":"markdown","source":"___ ","metadata":{},"id":"4ecf8b15-d0de-4abf-8822-1bef2b7d93e9"},{"cell_type":"markdown","source":"### function 2:  `get_plate_boundaries`\n\n**Code:** The body of `get_plate_boundaries` should \n\n- take as input a `.csv` file containing three columns (column 1: plate boundary name abbreviations, column 2: latitudes in degrees, column 3: longitudes in degrees) and read these columns into a dictionary. Each tectonic plate will have a key in the dictionary (the key should be the plate's name abbreviation), and the associated value for each key should be a $N\\times2$ array  with longitudes in the first column and latitudes in the second column. \n- The function should also raise an `IOError` and exit with a helpful error message if an exception is encountered inside of the function. \n\n**Usage:**\n```python\npb_dict = get_plate_boundaries(plates_file)  # returns a dictionary\n```\n**Inputs:** \n-  `plates_file`:  file containing plate boundary names, latitudes and longitudes.\n\n**Outputs:** \n- *pb_dict*: a dictionary with a key-value pair for each unique tectonic plate in the file.\n> - The keys for the dictionary should be the tectonic plate name abbreviation from the first column of the file.\n> - The corresponding value for the key should be a $N \\times 2$ array with longitudes for the plate in the first column, and latitudes in the second column. ","metadata":{},"id":"02afaee7-8c33-464d-a2e8-010377dd009c"},{"cell_type":"code","source":"# df = pd.read_csv(\"./all_boundaries.csv\")\n# plate = np.array(df.iloc[:, 0])\n# lat = np.array(df.iloc[:, 1])\n# lon = np.array(df.iloc[:, 2])\n# plates = dict()\n# plate_name = \"\"\n# for i in range(len(plate)):\n#     if (plate_name != plate[i]):\n        \n#         plate_name = plate[i]\n#         plates[plate_name] = np.array([[lon[i]], [lat[i]]])\n#     else:\n#         temp = plates[plate_name]\n#         plates[plate_name] = np.append(temp, np.array([[lon[i]], [lat[i]]]))\n#         plates[plate_name].reshape(-1, 2)\n        \n# print(plates[\"su\"].reshape(-1, 2))\n\n# a = np.array([[lat[0]], [lat[1]]])\n# print(plates.keys())\n# print(a)","metadata":{"trusted":true,"tags":[]},"execution_count":2,"outputs":[],"id":"b62f7ceb-1ac3-4623-a5de-aa9966c9a70e"},{"cell_type":"code","source":"# def get_plate_boundaries(plates_files):\n#     try:\n#         df = pd.read_csv(\"./all_boundaries.csv\")\n#         plate = np.array(df.iloc[:, 0])\n#         lat = np.array(df.iloc[:, 1])\n#         lon = np.array(df.iloc[:, 2])\n#         plates = dict()\n#         plate_name = \"\"\n#         for i in range(len(plate)):\n#             if (plate_name != plate[i]):   \n#                 plate_name = plate[i]\n#                 plates[plate_name] = np.array([[lon[i]], [lat[i]]])\n#             else:\n#                 temp = plates[plate_name]\n#                 plates[plate_name] = np.append(temp, np.array([[lon[i]], [lat[i]]]))\n#                 plates[plate_name].reshape(-1, 2)\n#         return plates\n#     except:\n#         raise IOError\n    ","metadata":{"trusted":true,"tags":[]},"execution_count":3,"outputs":[],"id":"c02a8450-f20c-47d5-9c7a-cdd3dda2d157"},{"cell_type":"markdown","source":" \n\n___ \n\n### function 3: `get_earthquakes`\n\n**Code:** The body of `get_earthquakes` should \n\n- read an input `.csv` file containing an arbitrary number of columns and rows into a `DataFrame` and return it.\n- The function should also raise an `IOError` and exit with a helpful error message if an exception is encountered inside of the function. \n\n\n**Usage:**\n```python\nearthquakes = get_earthquakes(filename)  # returns a DataFrame\n```\n**Inputs:** \n-  `filename`:  path to a `.csv` file. In practice this will be the IRIS earthquakes file. \n\n**Outputs:** \n- *earthquakes*: a `DataFrame` containing the contents of the `.csv` file\n\n\n*Note*: This function will be reusable for any problem involving a loading a `.csv` and outputting a `DataFrame` object. *Output variable name in italics is suggested name.* \n\n\n\n___ ","metadata":{},"id":"7e9bf093-b454-4abd-9285-e7cff164dbd5"},{"cell_type":"markdown","source":"### function 4:  `parse_earthquakes_to_np`\n\n**Code:** The body of `parse_earthquakes_to_np` should \n\n- take an input `DataFrame` containing earthquake data and extract the columns `Latitude`, `Longitude`, `Depth`, `Magnitude` into individual 1D arrays. \n- also extract the column `Time`, convert each time in times into `datetime` objects, and return a 1D array which contains the `datetime` objects converted from ISO format (see notebook `wk13_pandas_getfamiliar.ipynb` which shows how to do this if you are stuck). \n\n**Usage:**\n```python\nlats, lons, depths, magnitudes, times = parse_earthquakes_to_np(df)  \n```\n**Inputs:** \n-  `df`:  a `DataFrame` containing earthquake data.  This must contain the columns `Latitude`, `Longitude`, `Depth`, `Magnitude` and `Time` (may also contain others that are not parsed here). \n\n**Outputs:** \n- *lats*: numpy array of floats containing earthquake latitudes in degrees\n- *lons*: numpy array of floats containing earthquake longitudes in degrees\n- *depths*: numpy array of floats containing earthquake depths in km\n- *magnitudes*: numpy array of floats containing earthquake magnitudes (unitless)\n- *times*: numpy array of **datetime objects** containing earthquake dates/times\n\n***Note**: all inputs and outputs must be provided in the exact order as above. Output variable names in italics are suggested names.* ","metadata":{},"id":"48c5be4c-f743-4333-bc6d-0e9a9bb20e57"},{"cell_type":"code","source":"# from datetime import datetime\n# test = \"./IRIS_eq_010100_112422_mag4.csv\"\n# df = pd.read_csv(test)\n# def parse_earthquakes_to_np(df):\n#     lats = np.array(df[\"Latitude\"])\n#     lons = np.array(df[\"Longitude\"])\n#     depths = np.array(df[\"Depth\"])\n#     magnitudes = np.array(df[\"Magnitude\"])\n#     times_object = np.array(df[\"Time\"])\n#     times = pd.to_datetime(times_object)\n#     return lats, lons, depths, magnitudes, times\n\n# parse_earthquakes_to_np(df)\n","metadata":{"trusted":true,"tags":[]},"execution_count":4,"outputs":[],"id":"32732dd8-db36-437f-9e9a-96ea050b02b6"},{"cell_type":"markdown","source":"___ \n\n## C. Global Seismicity Characteristics:","metadata":{},"id":"5cb7081c-e3ee-4242-9a9a-b04b20a3b8d2"},{"cell_type":"markdown","source":"### C1. Preliminaries:\n- Open a new notebook and import your module `earthquake_fns`\n\n- Using the functions in `earthquake_fns`: \n> - Load the coastlines data set using `load_coastlines`.\n> - Load the plate boundaries data set using `load_plate_boundaries`.\n> - Load the entire earthquake data set into a `DataFrame` using `load_earthquakes`.\n> - Parse the `DataFrame` into arrays `lats`, `lons`, `depths`, `magnitudes`, `times` using `parse_earthquakes_to_np`.\n\n<br>\n\n\n- Write code to find the **largest magnitude** earthquake recorded this century. Identify and save the magnitude, date/time, latitude, longitude, depth of this earthquake in a new variable.  A dictionary would be a good data type to use to store the information about location, time, magnitude, depth that you find. You can use either numpy or pandas to accomplish this. Print your answer to your screen using a formatted string. \n\n- Similarly, find and save the magnitude, date/time, latitude, longitude, and depth of the **deepest** earthquake this century. Print your answer to your screen using a formatted string. ","metadata":{},"id":"8d145a79-f4ec-42b6-baca-e6034b89e252"},{"cell_type":"code","source":"import earthquake_fns as eq\n\nlongitudes, latitudes = eq.get_coastlines(\"./m_coasts.csv\")\nplate_dict = eq.get_plate_boundaries(\"./all_boundaries.csv\")\nearthquakes = eq.get_earthquakes(\"./IRIS_eq_010100_112422_mag4.csv\")\nlats, lons, depths, magnitudes, times = eq.parse_earthquakes_to_np(earthquakes)\n\n# print(earthquakes)\n# print(plate_dict.keys())\n# print(plate_dict[\"AP\"])","metadata":{"trusted":true,"tags":[]},"execution_count":5,"outputs":[],"id":"d3dba60f-4ba2-4d40-811a-8142ee44d087"},{"cell_type":"code","source":"largest_magnitude_index = np.argmax(magnitudes)\nlargest_magnitude = {\n    'magnitude': magnitudes[largest_magnitude_index],\n    'date/time': times[largest_magnitude_index],\n    'latitude': lats[largest_magnitude_index],\n    'longitude': lons[largest_magnitude_index] ,\n    'depth': depths[largest_magnitude_index],\n}\n\nprint(f\"Largest Magnitude: {largest_magnitude['magnitude']}\\n\", \n      f\"Date/Time: {largest_magnitude['date/time']}\\n\",\n      f\"Latitude:{largest_magnitude['latitude']}\\n\",\n      f\"Longitude: {largest_magnitude['longitude']}\\n\",\n      f\"Depths: {largest_magnitude['depth']}\")\n","metadata":{"trusted":true,"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","text":"Largest Magnitude: 9.1\n Date/Time: 2011-03-11 05:46:23\n Latitude:38.2963\n Longitude: 142.498\n Depths: 19.7\n","output_type":"stream"}],"id":"221ae86d-f69a-47af-8aea-7dfd5daf3bfb"},{"cell_type":"markdown","source":"### C2.  Make a Global Map: \n\n- Make a map of the locations of earthquakes listed in the last 2500 lines of the file.\n> -  Plot the earthquakes as gray filled dots.\n> -  Make sure your map has a 2:1 aspect ratio (because there are 360° of longitude and 180° of latitude) so that geography looks nice.\n\n- Add the world coastlines and plate boundaries to the map.\n\n- Add the locations of the largest magnitude and deepest quakes as distinct symbols/colors.\n\n- As usual make sure your map is scientifically useful (e.g. annotate information on the deepest, largest quakes) and aesthetically pleasing.\n\n- Find the dates spanned by the last 2500 lines in the data frame and give your plot a title indicating these e.g. `Seismicity from XX to YY`, where `XX` and `YY` are the dates you found.  See background reading notes in `project_lab2_background.pdf`: section **5. Coding notes** for converting datetime objects to strings.  \n\n\n\n### C3: Earthquakes in 2022: \n\n- Make second dataframe `df_2022` containing only earthquakes from 2022. (Consult `wk13_pandas_get_familiar.ipynb` for how to subselect from a dataframe). \n\n- Make a figure with 2 subplots side-by-side (1 row, 2 columns). \n> - Subplot 1: plot a histogram of quake depths in 2022.\n> - Subplot 2: plot a histogram of quake magnitudes in 2022.\n>\n> *Tip*: Experiment with the number of bins in the histogram and choose one that you think results in a nice figure.\n\n\n- For both subplots, mark a vertical dashed line that indicates the biggest and deepest quake for the entire dataset that you found in Section **C1** above and label it accordingly.","metadata":{},"id":"0717e372-c10b-467a-9c93-876c5be9d1ed"},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n\n# Submission Instructions\n\n###  You should submit: \n- Your python file `earthquake_fns` that is the module described in Part 1\n    \n- A single Jupyter notebook with ONLY 2 cells with contents as follows\n> - Cell 1: A markdown cell with your names and student numbers\n> - Cell 2: A code cell with your import statements, function calls, and code to produce the requested figures / analyses for **Section C**. \n\n- A `.pdf` file of your Jupyter Notebook which includes any output requested (figures, print statements, etc.)\n\n</div>","metadata":{},"id":"8bf16e8c"},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"286966ad-c1cd-4b5d-b1ba-6caf5ffb908c"}]}